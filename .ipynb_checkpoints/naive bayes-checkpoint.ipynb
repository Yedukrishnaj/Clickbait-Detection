{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3056cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "import string as s\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8466685e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>clickbait</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Should I Get Bings</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Which TV Female Friend Group Do You Belong In</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The New \"Star Wars: The Force Awakens\" Trailer...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This Vine Of New York On \"Celebrity Big Brothe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A Couple Did A Stunning Photo Shoot With Their...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            headline  clickbait\n",
       "0                                 Should I Get Bings          1\n",
       "1      Which TV Female Friend Group Do You Belong In          1\n",
       "2  The New \"Star Wars: The Force Awakens\" Trailer...          1\n",
       "3  This Vine Of New York On \"Celebrity Big Brothe...          1\n",
       "4  A Couple Did A Stunning Photo Shoot With Their...          1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cb_data= pd.read_csv('clickbait_data.csv')\n",
    "cb_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2b96af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=cb_data.headline\n",
    "y=cb_data.clickbait\n",
    "train_x,test_x,train_y,test_y=train_test_split(x,y,test_size=0.25,random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d456a606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of elements in training set\n",
      "24000\n",
      "No. of elements in testing set\n",
      "8000\n"
     ]
    }
   ],
   "source": [
    "print(\"No. of elements in training set\")\n",
    "print(train_x.size)\n",
    "print(\"No. of elements in testing set\")\n",
    "print(test_x.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "096a0d02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3882        7 Facts About History That Will Blow Your Mind\n",
       "3859            Only Take This Quiz If You're Taylor Swift\n",
       "28753    England and New Zealand vie for Women's Cricke...\n",
       "4323     12 Glorious Baileys Cocktails That Will Deligh...\n",
       "10799    This Corgi And Baby Are Best Friends And It's ...\n",
       "14256    13 Things You Should Never Say To A Latina Whe...\n",
       "19466    Spelbound declared winner of Britain's Got Tal...\n",
       "8380                   Are You More NSYNC Or One Direction\n",
       "22670               BP Nominates Russian to Oversee TNK-BP\n",
       "20553        Facing German Suffering, and Not Looking Away\n",
       "Name: headline, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38228330",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3882     1\n",
       "3859     1\n",
       "28753    0\n",
       "4323     1\n",
       "10799    1\n",
       "14256    1\n",
       "19466    0\n",
       "8380     1\n",
       "22670    0\n",
       "20553    0\n",
       "Name: clickbait, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7349f6ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26171    Sixteen Christian converts arrested in Iran; f...\n",
       "16224    Hiring of Isiah Thomas Angers Some F.I.U. Facu...\n",
       "27534       Fußball-Bundesliga 2007–08: Matchday 1 roundup\n",
       "27304       Taco Bell mascot Gidget dies from stroke at 15\n",
       "24836    China Takes Heavy Criticism Over Software Dire...\n",
       "Name: headline, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b041e2cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26171    0\n",
       "16224    0\n",
       "27534    0\n",
       "27304    0\n",
       "24836    0\n",
       "Name: clickbait, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c6d6144",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization(text):\n",
    "    lst=text.split()\n",
    "    return lst\n",
    "train_x=train_x.apply(tokenization)\n",
    "test_x=test_x.apply(tokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "648fc422",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowercasing(lst):\n",
    "    new_lst=[]\n",
    "    for i in lst:\n",
    "        i=i.lower()\n",
    "        new_lst.append(i)\n",
    "    return new_lst\n",
    "train_x=train_x.apply(lowercasing)\n",
    "test_x=test_x.apply(lowercasing)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a12b2307",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuations(lst):\n",
    "    new_lst=[]\n",
    "    for i in lst:\n",
    "        for j in s.punctuation:\n",
    "            i=i.replace(j,'')\n",
    "        new_lst.append(i)\n",
    "    return new_lst\n",
    "train_x=train_x.apply(remove_punctuations)\n",
    "test_x=test_x.apply(remove_punctuations)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f43adcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_numbers(lst):\n",
    "    nodig_lst=[]\n",
    "    new_lst=[]\n",
    "    for i in lst:\n",
    "        for j in s.digits:    \n",
    "            i=i.replace(j,'')\n",
    "        nodig_lst.append(i)\n",
    "    for i in nodig_lst:\n",
    "        if i!='':\n",
    "            new_lst.append(i)\n",
    "    return new_lst\n",
    "train_x=train_x.apply(remove_numbers)\n",
    "test_x=test_x.apply(remove_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7d4816b5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6da0526d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\YEDU\n",
      "[nltk_data]     KJ\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "601ad09f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All stopwords of English language \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"i, me, my, myself, we, our, ours, ourselves, you, you're, you've, you'll, you'd, your, yours, yourself, yourselves, he, him, his, himself, she, she's, her, hers, herself, it, it's, its, itself, they, them, their, theirs, themselves, what, which, who, whom, this, that, that'll, these, those, am, is, are, was, were, be, been, being, have, has, had, having, do, does, did, doing, a, an, the, and, but, if, or, because, as, until, while, of, at, by, for, with, about, against, between, into, through, during, before, after, above, below, to, from, up, down, in, out, on, off, over, under, again, further, then, once, here, there, when, where, why, how, all, any, both, each, few, more, most, other, some, such, no, nor, not, only, own, same, so, than, too, very, s, t, can, will, just, don, don't, should, should've, now, d, ll, m, o, re, ve, y, ain, aren, aren't, couldn, couldn't, didn, didn't, doesn, doesn't, hadn, hadn't, hasn, hasn't, haven, haven't, isn, isn't, ma, mightn, mightn't, mustn, mustn't, needn, needn't, shan, shan't, shouldn, shouldn't, wasn, wasn't, weren, weren't, won, won't, wouldn, wouldn't\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"All stopwords of English language \")\n",
    "\", \".join(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f78ac1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(lst):\n",
    "    stop=stopwords.words('english')\n",
    "    new_lst=[]\n",
    "    for i in lst:\n",
    "        if i not in stop:\n",
    "            new_lst.append(i)\n",
    "    return new_lst\n",
    "\n",
    "train_x=train_x.apply(remove_stopwords)\n",
    "test_x=test_x.apply(remove_stopwords)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1b8490a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_spaces(lst):\n",
    "    new_lst=[]\n",
    "    for i in lst:\n",
    "        i=i.strip()\n",
    "        new_lst.append(i)\n",
    "    return new_lst\n",
    "train_x=train_x.apply(remove_spaces)\n",
    "test_x=test_x.apply(remove_spaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "919115a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3882                          [facts, history, blow, mind]\n",
       "3859                    [take, quiz, youre, taylor, swift]\n",
       "28753    [england, new, zealand, vie, womens, cricket, ...\n",
       "4323     [glorious, baileys, cocktails, delight, tasteb...\n",
       "10799     [corgi, baby, best, friends, ridiculously, cute]\n",
       "Name: headline, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "344c6e33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26171    [sixteen, christian, converts, arrested, iran,...\n",
       "16224    [hiring, isiah, thomas, angers, fiu, faculty, ...\n",
       "27534            [fußballbundesliga, –, matchday, roundup]\n",
       "27304           [taco, bell, mascot, gidget, dies, stroke]\n",
       "24836    [china, takes, heavy, criticism, software, dir...\n",
       "Name: headline, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b4c5a1e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\YEDU\n",
      "[nltk_data]     KJ\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5a520cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer=nltk.stem.WordNetLemmatizer()\n",
    "def lemmatzation(lst):\n",
    "    new_lst=[]\n",
    "    for i in lst:\n",
    "        i=lemmatizer.lemmatize(i)\n",
    "        new_lst.append(i)\n",
    "    return new_lst\n",
    "train_x=train_x.apply(lemmatzation)\n",
    "test_x=test_x.apply(lemmatzation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2cce1a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x=train_x.apply(lambda x: ''.join(i+' ' for i in x))\n",
    "test_x=test_x.apply(lambda x: ''.join(i+' ' for i in x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ccbfbacd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fact': 1,\n",
       " 'history': 1,\n",
       " 'blow': 1,\n",
       " 'mind': 1,\n",
       " 'take': 2,\n",
       " 'quiz': 1,\n",
       " 'youre': 1,\n",
       " 'taylor': 1,\n",
       " 'swift': 1,\n",
       " 'england': 1,\n",
       " 'new': 2,\n",
       " 'zealand': 1,\n",
       " 'vie': 1,\n",
       " 'woman': 1,\n",
       " 'cricket': 1,\n",
       " 'world': 1,\n",
       " 'cup': 1,\n",
       " 'glorious': 1,\n",
       " 'bailey': 1,\n",
       " 'cocktail': 1,\n",
       " 'delight': 1,\n",
       " 'tastebud': 1,\n",
       " 'corgi': 1,\n",
       " 'baby': 1,\n",
       " 'best': 1,\n",
       " 'friend': 1,\n",
       " 'ridiculously': 1,\n",
       " 'cute': 1,\n",
       " 'thing': 2,\n",
       " 'never': 1,\n",
       " 'say': 1,\n",
       " 'latina': 1,\n",
       " 'dating': 1,\n",
       " 'spelbound': 1,\n",
       " 'declared': 1,\n",
       " 'winner': 1,\n",
       " 'britain': 1,\n",
       " 'got': 1,\n",
       " 'talent': 1,\n",
       " 'nsync': 1,\n",
       " 'one': 1,\n",
       " 'direction': 1,\n",
       " 'bp': 1,\n",
       " 'nominates': 1,\n",
       " 'russian': 1,\n",
       " 'oversee': 1,\n",
       " 'tnkbp': 1,\n",
       " 'facing': 1,\n",
       " 'german': 1,\n",
       " 'suffering': 1,\n",
       " 'looking': 1,\n",
       " 'away': 1,\n",
       " 'downturn': 1,\n",
       " 'marketer': 1,\n",
       " 'still': 1,\n",
       " 'rely': 1,\n",
       " 'product': 1,\n",
       " 'hurricane': 1,\n",
       " 'dean': 1,\n",
       " 'make': 2,\n",
       " 'second': 1,\n",
       " 'landfall': 1,\n",
       " 'weakens': 1,\n",
       " 'category': 1,\n",
       " 'storm': 1,\n",
       " 'tim': 1,\n",
       " 'hortons': 1,\n",
       " 'employee': 1,\n",
       " 'secretly': 1,\n",
       " 'wished': 1,\n",
       " 'knew': 1,\n",
       " 'single': 1,\n",
       " 'people': 1,\n",
       " 'honest': 1,\n",
       " 'holiday': 1,\n",
       " 'photo': 1,\n",
       " 'bomb': 1,\n",
       " 'blast': 1,\n",
       " 'iraqi': 1,\n",
       " 'football': 1,\n",
       " 'match': 1,\n",
       " 'kill': 1,\n",
       " 'many': 1,\n",
       " 'wounded': 1,\n",
       " 'mark': 1,\n",
       " 'ruffalo': 1,\n",
       " 'couldnt': 1,\n",
       " 'stop': 1,\n",
       " 'fanboying': 1,\n",
       " 'year': 1,\n",
       " 'baftas': 1,\n",
       " 'picture': 1,\n",
       " 'prove': 1,\n",
       " 'everyone': 1,\n",
       " 'totally': 1,\n",
       " 'faking': 1,\n",
       " 'stuffed': 1,\n",
       " 'potato': 1,\n",
       " 'need': 1,\n",
       " 'asap': 1,\n",
       " 'cara': 1,\n",
       " 'delevignes': 1,\n",
       " 'dog': 1,\n",
       " 'glamorous': 1,\n",
       " 'day': 1,\n",
       " 'u': 1,\n",
       " 'ever': 1,\n",
       " 'robotic': 1,\n",
       " 'car': 1,\n",
       " 'successfully': 1,\n",
       " 'complete': 1,\n",
       " 'mile': 1,\n",
       " 'darpa': 1,\n",
       " 'grand': 1,\n",
       " 'challenge': 1,\n",
       " 'race': 1}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_dist={}\n",
    "for i in train_x.head(20):\n",
    "    x=i.split()\n",
    "    for j in x:\n",
    "        if j not in freq_dist.keys():\n",
    "            freq_dist[j]=1\n",
    "        else:\n",
    "            freq_dist[j]+=1\n",
    "freq_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "49167690",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf=TfidfVectorizer()\n",
    "train_1=tfidf.fit_transform(train_x)\n",
    "test_1=tfidf.transform(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "72921803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features extracted\n",
      "18248\n",
      "\n",
      "The 100 features extracted from TF-IDF \n",
      "['aa', 'aaevpc', 'aaron', 'ab', 'abandon', 'abandoned', 'abandoning', 'abba', 'abbas', 'abbey', 'abbott', 'abby', 'abc', 'abdallahi', 'abdelbaset', 'abdicates', 'abduct', 'abducted', 'abduction', 'abductor', 'abdul', 'abdullah', 'abdulmutallab', 'abel', 'abercrombie', 'abhishek', 'abhisit', 'abide', 'ability', 'abin', 'abitibibowater', 'abject', 'abkhazia', 'ablaze', 'able', 'aboard', 'abolish', 'abombs', 'aborigine', 'aborted', 'abortion', 'abortionrights', 'aboulhosn', 'abound', 'abraham', 'abramoff', 'abrams', 'abroad', 'abrogates', 'abse', 'absence', 'absentee', 'absolute', 'absolutely', 'absolutereturn', 'absorbs', 'abstention', 'abstinence', 'absurd', 'absurdity', 'absurdly', 'abu', 'abuela', 'abuelita', 'abundant', 'abuse', 'abused', 'abuserelated', 'abusing', 'abusive', 'ac', 'academia', 'academic', 'academy', 'acapella', 'acc', 'accelerates', 'accelerating', 'accelerator', 'accent', 'accenture', 'accept', 'acceptance', 'accepted', 'accepting', 'accepts', 'access', 'accessible', 'accession', 'accessory', 'accident', 'accidental', 'accidentally', 'accolade', 'accord', 'according', 'account', 'accountability', 'accountant', 'accounted']\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of features extracted\")\n",
    "print(len(tfidf.get_feature_names()))\n",
    "print()\n",
    "print(\"The 100 features extracted from TF-IDF \")\n",
    "print(tfidf.get_feature_names()[:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9805b75d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train set (24000, 18248)\n",
      "Shape of test set (8000, 18248)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of train set\",train_1.shape)\n",
    "print(\"Shape of test set\",test_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "60eb8842",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_arr=train_1.toarray()\n",
    "test_arr=test_1.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "83355dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_MN=MultinomialNB()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "455d6f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first 20 actual labels:  [0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1]\n",
      "first 20 predicted labels:  [0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "NB_MN.fit(train_arr,train_y)\n",
    "pred=NB_MN.predict(test_arr)\n",
    "print('first 20 actual labels: ',test_y.tolist()[:20])\n",
    "print('first 20 predicted labels: ',pred.tolist()[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9d0e7b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score of the model\n",
      "0.9632343959936486\n",
      "Accuracy of the model\n",
      "0.962375\n",
      "Accuracy of the model in percentage\n",
      "96.2375 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score,accuracy_score\n",
    "print(\"F1 score of the model\")\n",
    "print(f1_score(test_y,pred))\n",
    "print(\"Accuracy of the model\")\n",
    "print(accuracy_score(test_y,pred))\n",
    "print(\"Accuracy of the model in percentage\")\n",
    "print(accuracy_score(test_y,pred)*100,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9e208ed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "[[3756  169]\n",
      " [ 132 3943]]\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.96      0.96      3925\n",
      "           1       0.96      0.97      0.96      4075\n",
      "\n",
      "    accuracy                           0.96      8000\n",
      "   macro avg       0.96      0.96      0.96      8000\n",
      "weighted avg       0.96      0.96      0.96      8000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(\"Confusion Matrix\")\n",
    "print(confusion_matrix(test_y,pred))\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(\"Classification Report\")\n",
    "print(classification_report(test_y,pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "81c6378e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model in percentage\n",
      "95.7125 %\n",
      "Confusion Matrix\n",
      "[[3774  151]\n",
      " [ 192 3883]]\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.96      0.96      3925\n",
      "           1       0.96      0.95      0.96      4075\n",
      "\n",
      "    accuracy                           0.96      8000\n",
      "   macro avg       0.96      0.96      0.96      8000\n",
      "weighted avg       0.96      0.96      0.96      8000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf=TfidfVectorizer(ngram_range=(1,3),max_features=6500)\n",
    "train_2=tfidf.fit_transform(train_x)\n",
    "test_2=tfidf.transform(test_x)\n",
    "\n",
    "NB_MN.fit(train_2.toarray(),train_y)\n",
    "pred2=NB_MN.predict(test_2.toarray())\n",
    "\n",
    "print(\"Accuracy of the model in percentage\")\n",
    "print(accuracy_score(test_y,pred2)*100,\"%\")\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(\"Confusion Matrix\")\n",
    "print(confusion_matrix(test_y,pred2))\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(\"Classification Report\")\n",
    "print(classification_report(test_y,pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efa41ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
